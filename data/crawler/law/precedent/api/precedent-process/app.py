from kafka import KafkaConsumer, KafkaProducer
import json
from bs4 import BeautifulSoup
import os
import requests
import re
from langchain import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv
load_dotenv()


KAFKA_BOOTSTRAP_SERVER = os.environ.get("KAFKA_BOOTSTRAP_SERVER")
GPT_KEY = os.environ.get("GPT_KEY")




device = "cuda" if os.environ.get("DEVICE") in ["gpu", "GPU"] else "cpu"

KAFKA_CONSUME_TOPIC = "precedent-api-list"
# KAFKA_PRODUCE_TOPIC = "precedent_proceesed_with_vector"
KAFKA_PRODUCE_TOPIC = "precedent-process"
# FILE_STORE_PATH = "/app/data"
FILE_STORE_PATH = "./data"

assert KAFKA_BOOTSTRAP_SERVER != None
assert GPT_KEY != None

print(KAFKA_BOOTSTRAP_SERVER,GPT_KEY,device)

consumer = KafkaConsumer(
    KAFKA_CONSUME_TOPIC,
    group_id="precedent_api_list_crawler",  # Consumer ê·¸ë£¹ ID
    bootstrap_servers=KAFKA_BOOTSTRAP_SERVER,
    auto_offset_reset='earliest',  # ì†Œë¹„ì ê·¸ë£¹ì´ ì²˜ìŒìœ¼ë¡œ ì˜¤í”„ì…‹ì„ ì„¤ì •í•  ë•Œ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ì„¤ì •
    max_poll_records=10
)


producer = KafkaProducer(
    acks=0,  # ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œì— ëŒ€í•œ ì²´í¬
    compression_type='gzip',  # ë©”ì‹œì§€ ì „ë‹¬í•  ë•Œ ì••ì¶•(None, gzip, snappy, lz4 ë“±)
    bootstrap_servers=KAFKA_BOOTSTRAP_SERVER,  # ì „ë‹¬í•˜ê³ ì í•˜ëŠ” ì¹´í”„ì¹´ ë¸Œë¡œì»¤ì˜ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
    value_serializer=lambda x: json.dumps(x).encode('utf-8')  # ë©”ì‹œì§€ì˜ ê°’ ì§ë ¬í™”
)

model_name = "jhgan/ko-sroberta-multitask"  # (KorNLU ë°ì´í„°ì…‹ì— í•™ìŠµì‹œí‚¨ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸)


model_kwargs = {'device': device}
encode_kwargs = {'normalize_embeddings': False}

embedding_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)


template = """ë‹¤ìŒê³¼ ê°™ì€ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤.
ë§Œì•½ ë‹µì„ ëª¨ë¥´ë©´ nullì´ë¼ê³  í•˜ê³  ë‹µì„ ì§€ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
ë‹µë³€ì€ í…œí”Œë¦¿ì— ë§ê²Œ json í˜•ì‹ìœ¼ë¡œ ëŒ€ë‹µí•˜ì‹œì˜¤.
ë§¨ ì²« 3ì¤„ê¹Œì§€ ë“±ì¥í•˜ëŠ” íŒê²°ë²ˆí˜¸ëŠ” ì›ì‹¬íŒê²°ë²ˆí˜¸ê°€ ì•„ë‹˜.
ë§¥ë½: {content}
ì§ˆë¬¸ : ì·¨ì§€, ì›ê³ ì˜ ì£¼ì¥, í”¼ê³ ì˜ ì£¼ì¥, ì›ê³ , í”¼ê³ , ì¼ì–´ë‚œ ì‚¬ì‹¤, ë²•ì›ì˜ íŒë‹¨, ì›ì‹¬íŒê²°ë²ˆí˜¸, ì£¼ë¬¸, ìš”ì•½, í‚¤ì›Œë“œë“¤
í…œí”Œë¦¿ :
[
"purport": ì·¨ì§€
"opinion": [
"plaintiff": ì›ê³ ì˜ ì£¼ì¥,
"defendant": í”¼ê³ ì˜ ì£¼ì¥,
],
"parties": [
"plaintiff": ì›ê³ ,
"defendant": í”¼ê³ ,
],
"fact": ì¼ì–´ë‚œ ì‚¬ì‹¤,
"judgement": ë²•ì›ì˜íŒë‹¨,
"originalJudgement": [
"caseNumber": ì›ì‹¬íŒê²°ë²ˆí˜¸
],
"conclusion": ì£¼ë¬¸,
"summary": ìš”ì•½,
"keywordList": [í‚¤ì›Œë“œë“¤]
]"""

prompt = PromptTemplate.from_template(template)
llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k",
                 temperature=0, openai_api_key=GPT_KEY)
chain = prompt | llm | JsonOutputParser()


def get_response_by_precSeq(precSeq):
    params = {
        "precSeq": precSeq,
        "mode": 0
    }
    url = "https://www.law.go.kr/LSW/precInfoP.do"
    response = requests.get(url, params=params)
    return response


def parse_raw_html(response):
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    data = soup.find("div", {"id": "contentBody"})
    return str(data)


def save_html(file_name, html):
    with open(f"{FILE_STORE_PATH}/html/{file_name}.html", "w", encoding="utf8") as f:
        f.write(html)


def save_json(file_name, json_data):
    with open(f"{FILE_STORE_PATH}/json/{file_name}.json", "w",encoding="utf8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=4)


def parse_info(json_data):
    info = {}

    info['caseNumber'] = json_data['ì‚¬ê±´ë²ˆí˜¸']
    info['caseName'] = json_data['ì‚¬ê±´ëª…']
    info['judgementDate'] = json_data['ì„ ê³ ì¼ì']
    info["type"] = {}
    info['type']['incident'] = json_data['ì‚¬ê±´ì¢…ë¥˜ëª…']
    info['type']['courtName'] = json_data['ë²•ì›ëª…']
    info['type']['verdict'] = json_data['ì„ ê³ ']
    return info


def parse_relate(html):
    # ì°¸ì¡° íŒë¡€
    soup = BeautifulSoup(html, "html.parser")
    relateLawSet = set()
    relatePrecedentSet = set()
    linkList = soup.findAll("a", {"href": "#AJAX"})
    before = ""
    for i in linkList:
        relate = i['onclick']
        relate = relate.replace(");", "").strip()
        if (relate.startswith("javascript:fncLawPop")):
            relate = relate.replace("javascript:fncLawPop(", "")
            try:
                start, end = re.search(pattern, i.text).span()
                name = i.text[start:]
                before = name.strip()
            except:
                name = before + i.text
            relate = relate.replace("''", "")
            relate = f"[{relate}]ğŸ‘µ{name}"
            relateLawSet.add(relate)
    #         relate = eval(relate)
        else:
            relate = relate.replace(
                "javascript:showCase(", "").strip().strip("'")
            relatePrecedentSet.add(relate)

    relatePrecedentList = [{"caseNumber": i} for i in relatePrecedentSet]

    relateLawList = []
    for i in relateLawSet:
        temp, j = i.split("ğŸ‘µ")
        try:
            i = eval(temp)
            t = f"{i[0]} {j}"
            # print(i)
            relateLawList.append(
                {
                    "lawName": t,
                    "searchNumber": i[2],
                    "searchName": i[0],
                    "searchType": i[1],
                    "searchKey": i[3]
                }
            )
        except:
            pass
    relateLawList = sorted(relateLawList, key=lambda x: x["lawName"])

    return {
        "lawList": relateLawList,
        "precedentList": relatePrecedentList

    }


def get_html_text(html):
    soup = BeautifulSoup(html)
    text = soup.get_text()
    text = re.sub("\s{2,}", "\n", text)
    return text


while True:
    for message in consumer:
        key = message.key.decode('utf-8') if message.key else None
        value = message.value.decode('utf-8') if message.value else None
        try:
            value = json.loads(value)
            precSeq = value['íŒë¡€ì¼ë ¨ë²ˆí˜¸']
            response = get_response_by_precSeq(precSeq)
            html = parse_raw_html(response)
            relate = parse_relate(html)
            info = parse_info(value)
            caseNumber = info["caseNumber"]
            isPdf = False
            body = html
            text = get_html_text(html)
            save_html(caseNumber, html)
            vector = embedding_model.embed_query(text)

            response = {

                "info": info,
                "relate": relate,
                "caseNumber": caseNumber,
                "vector": vector,
                "isPdf": isPdf,
                "body": body
            }
            out = chain.invoke({"content": text})
            for key in out.keys():
                response[key] = out[key]
            producer.send(KAFKA_PRODUCE_TOPIC, value=response)
            producer.flush()
            save_json(caseNumber, response)

        except KeyboardInterrupt:
            exit(0)
        except Exception as ex:
            pass